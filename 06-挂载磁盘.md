## 将磁盘挂载到容器

**注意: $1M=1000*1000$ $1Mi=1024*1024$**
    
pod 中的容器 container 的文件系统独立，重启后数据内容丢失，需要一个pod级的持久化存储。
        
1. #### 卷介绍
    * 存储卷 volume 是 pod 的一部分，与 pod 共享生命周期，在容器重启时，可以从卷中识别上一个容器的数据和文件，且卷可以被pod中的多个容器识别。
    * 容器要使用卷，必须事先挂载，而且可以挂载在容器文件系统的任意位置。
    * 例子：
        * 服务器WebSeever，读写
        * 内容生成ContentAgent，读
        * 日志转换LogRotator， 写
    ```mermaid
    graph LR
    A(websever) --read--> E(存储卷publicHtml)
    A(websever) --write--> F(存储卷logVol)
    B(ContentAgent) --read--> E
    C(LogRotator) --write--> F
    ```
    * **卷类型**，先看前四种
        * emptyDir：存储临时数据的简单空目录
        * hostPath：将目录从**工作节点**的文件系统挂载到 pod 中
        * gitRepo：检出 git 仓库来初始化卷
        * nfs：挂载到 pod 的 NFS 共享卷
        * configMap, secret downwardAPI：不是用于存储数据，而是将 kubernetes 的部分资源和集群信息公开给 pod，
        * persistentVolumeClaim: 使用预置或者动态配置的持久化存储类型
        * gcePersistentDisk (Google 高效能型存储磁盘卷）, awsElasticBlockStore (AmazonWeb 服务弹性块存储卷）,azureDisk (MicrosotAzure 磁盘卷）：云服务商提供
2. #### 卷的使用：在容器之间共享（pod级别）
    * **emptyDir卷**，同一个pod中的容器共享文件，或者用于写入磁盘（容器的文件系统未必可写）
        * 构建一个luksa/fortune镜像，见文末附录
        * 构造pod，用**luksa/fortune生成HTML**，用**Ngix做server**，原理是fortune每10s把一句名言写入index.html，nginx服务器收到请求时会响应index.html
            ```shell
            #./fortune-pod.yaml
            apiVersion: v1
            kind: Pod
            metadata:
                name: fortune
            spec:
                containers:
                -   image: luksa/fortune
                    name: html-generator
                    volumeMounts:
                    -   name: html # 使用名为 html 的卷
                        mountPath: /var/htdocs # 卷挂载位置
                -   image: nginx:alpine
                    name: html-server
                    volumeMounts:
                    -   name: html
                        mountPath: /usr/share/nginx/html # ngnix默认服务文件目录
                        readOnly: true  #设为只读
                    ports:
                    -   containerPort: 80
                        protocol: TCP
                volumes:
                -   name: html # 声明一个名为 html 的卷，自动绑定 Pod 的生命周期,挂载在两个容器中
                    emptyDir: {}      
            ```
            ```shell
            $ sudo kubectl create -f fortune-pod.yaml #创建pod
            $ sudo kubectl port-forward fortune 8080:80 #转发本机端口到pod  #--address 0.0.0.0 指定ip
            $ curl 127.0.0.1:80 
            Your business will go through a period of considerable expansion.
            ```
        * 指定存储介质，卷在磁盘上，可以指定在内存上
            ```shell
            volumes:
            -   name: html
                emptyDir:
                    medium: Memory #指定卷在内存中
            ```
    * **使用git仓库做卷**
        gitRepo本质上也是一个emptyDir，pod启动时，容器启动前克隆git仓库的特定分支，创建后不会随git repo更新，除非由**RC管理且删除原来的pod**，或者新建一个进程进行clone
        ```shell
        #./gitrepo-volume-pod.yaml
        apiVersion: v1
        kind: Pod
        metedata:
            name: gitrepo-volume-pod
            spec:
                containers:
                -   image: nginx:alpine
                    name: web-sever
                    volumeMounts:
                    -   name: html
                        Path: /usr/share/nginx/html
                        readOnly: true
                    ports:
                    -   containerPort: 80
                        protocol: TCP
                volumes:
                -   name: html
                    gitRepo: #指定卷类型
                        repository: https://github.com/luksa/website-example.git
                        revision: master #分支
                        directory: . #克隆到卷的根目录
        ```
        * sidecar容器，应该为git同步进程新建一个容器，而不是加入主应用程序，这会使其逻辑过于复杂。可以在dockerHub搜索git syc。
        * 私有repo，kubernetes的共识是维护giRepo的简单性，故而不添加SSH克隆协议，可以用sidecar容器实现
    
2. #### 卷的使用访问节点上的文件
    通常pod忽略结点，但是一些系统级pod需要访问节点文件或者通过其文件系统访问设备，通常由**Daemonset管理**
    * **hostPath**
        * emptyDir和gitRepo在pod删除时删除，而hostPath不会。重启的新pod使用同样的卷，即上一个pod的数据（调度到同一个节点的情况下）。
        * 通常用于访问节点数据，还有单节点集群的持久化存储。切勿用于持久化跨pod数据
            ```shell
            # 查看pod的挂载情况
            $ sudo kubectl get po kube-addon-manager-minikube -n kube-system -o yaml
            ```
            ```yaml
            # 构造mongodb挂载实例
            # ./mongodb-pod-hostpath.yaml
            apiVersion: v1
            kind: Pod
            metadata:
                name: mongodb 
            spec:
                volumes: # 挂载卷
                -   name: mongodb-data
                    hostPath: # 卷类型为hospath
                        path: /tmp/mongodb #路径
                containers:
                -   image: mongo
                    name: mongodb
                    volumeMounts:
                    -   name: mongodb-data
                        mountPath: /data/db #挂载路径
                    ports:
                    -   containerPort: 27017
                        protocol: TCP
            ```
            ```shell
            $ sudo kubectl exec -it mongodb mongo #连接到mongodb容器
            >use mystore
            >db.foo.insert({name:'foo'})
            >db.foo.find() #插入成功
            { "_id" : ObjectId("60df37d58f50442ea9c64042"), "name" : "foo" }
            #删除pod后重新create，连入pod
            $ sudo kubectl exec -it mongodb mongo #连接到mongodb容器
            >use mystore
            >db.foo.find() #新的pod仍然可以查到数据
            { "_id" : ObjectId("60df37d58f50442ea9c64042"), "name" : "foo" }
            ```
3. **持久化存储**
    通常存储在```NAS```中
    * Google，使用GCE
        ```shell
        #创建磁盘
        $ gcloud compute disks create --size=lGiB --zone=europe-west1-b mongodb
        ```
    * amazom AWS弹性存储模块
    * 挂载NFS卷，如果集群在服务器上，可以挂载一个NFS共享，指定NFS服务器和路径
        ```shell
        volumes:
        -   name: mongodb-data
            nfs:
                server: 1.2.3.4 #NFS服务器ip
                path: /spme/path #服务器提供的路径
        ```

4. #### 从底层存储解耦pod
    在发布pod的时候需要配置集群中的存储的真实结构，带来了一定的耦合性，而且与 kubernetes 向应用与开放人员隐藏基础设施的原则不符合。
    * 持久卷（PersistentVolume）与持久卷声明（PV Claim）
        * 集群管理员：创建某种类型的网络存储；创建PV提交，可以指定大小和访问方式
        * 开发者：创建PVC清单，提交；kubernetes找到匹配的PV并绑定到PVC；可以在之后的pod中引用PVC了
    * 创建持久卷PV
        ```shell
        # ./mongodb-pv-hostpath.yaml
        apiVersion: v1
        kind: PersistentVolume
        metadata:
            name: mongodb-pv
        spec:
            capacity: 
                storage: 500M # 我设为500Mi，书上为1Gi
            accessModes:
                -   ReadWriteOnce #单个点读写
                -   ReadOnlyMany #多个点同时可读
            persistentVolumeReclaimPolicy: Retain
            hostPath:
                path: /tmp/mongodb
        ```
        ```shell
        $ sudo kubectl get pv
        NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
        mongodb-pv   500M       RWO,ROX        Retain           Available                                   8m41s
        #此时status是available可用的
        # Retain：当 PV 被删除后 PV 的内容会被保留。相应策略还有 Delete 和 Recycle，回收策略可以动态改变
        ```
        持久卷PV没有命名空间，是共享的，PV声明是有命名空间的，访问模式有三种：
        * RWO：ReadWriteOnce，仅允许单个节点挂载读写。
        * ROX：ReadOnlyMany，允许多个节点挂载只读 。
        * RWX：ReadWriteMany，允许多个节点挂载读写这个卷 。
    * 创建持久卷声明：
        在创建持久化存储之前的 pod 之前，要先建一个持久化声明PVC，二者独立，保证po重新调度也可以使用PVC
        ```shell
        # ./mongodb-pvc.yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
            name: mongodb-pvc 
        spec:
            resources:
                requests:
                    storage: 500M #1Gi也可，请求500M的空间
            accessModes:
            -   ReadWriteOnce #只允许单个用户读写
            storageClassName: "" #后续动态配置提到,这里是不调用provisioner，即不使用SC
        ```
        该PVC申请500M的RWO的持久化存储，kubernetes会将其绑定到一个合适的持久卷上 
        ```shell
        $ sudo kubectl get pvc
        NAME          STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
        mongodb-pvc   Bound    mongodb-pv   500M       RWO,ROX                       9s
        $ sudo kubectl get pv
        NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
        mongodb-pv   500M       RWO,ROX        Retain           Bound    default/mongodb-pvc                           25m
        ```
        此时pv的CLAIM表示持久卷被挂载到**default空间**的mongodb-pvc上，且不可以被其他pvc申明，除非被该pvc释放。
    * 在pod中使用PVC
        ```shell
        #./mongodb-pod-pvc.yaml
        apiVersion: v1
        kind: Pod
        metadata:
            name: mongodb
        spec:
            containers:
            -   image: mongo
                name: mongodb
                volumeMounts:
                -   name: mongodb-data
                    mountPath: /data/db
                ports:
                -   containerPort: 27017
                    protocol: TCP
            volumes:
            -   name:mongodb-data
                persistentVolumeClaim:
                    claimName: mongodb-pvc
        ```
        ```shell
        $ sudo kubectl exec -it mongodb mongo
        > use mystroe
        > db.goo.find()
        { "_id" : ObjectId("60df37d58f50442ea9c64042"), "name" : "foo" }
        #数据仍然存在，因为持久卷与之前挂载的hostpath路径一致/tmp/mongodb
        ```
    * 持久卷的回收，删除pvc后，pv处于released状态，因为可能有前一个声明者的数据，此时新建pvc会处于pending状态。
        ```shell
        $ sudo kubectl delete pod mongdb
        $ sudo kubectl delete pvc mongdb-pvc
        $ sudo kubectl get pv
        NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS   REASON   AGE
        mongodb-pv   500M       RWO,ROX        Retain           Released   default/mongodb-pvc                           10h
        # pv的状态status处于released，不可以分配
        ```
    * 持久卷PV回收
        将 ersistentVolumeReclaimPolicy 设置为：
        * Retain，表明在pv释放后保留数据。手动回收，删除和重新创建PV
        * Recycle，自动删除数据，使PV可以被重新声明
        * Delete 删除底层存储,PVC删除PV自动删除

6. #### 持久卷动态配置StorageClass
    * 管理员自行创建持久卷配置，定义StorageClass对象，用户在PVC中引用SC，系统会自动创建PV，这样就不用管理员每次手动创建一个PV。SC不限定于命名空间。
    * SC制定了提供PV的配置程序，已经要传递的参数
        ```shell
        #./storage-fast-hostpath.yaml
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: fast
        provisioner: k8s.io/minikube-hostpath #配置持久化卷的参数
        parameters: # 传递给parameters的参数
          type: pd-ssd
        ```
    * 创建pvc引用SC
        ```shell
        #./mongodb-pvc-db.yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
            name: mongodb-pvc
        spec:
            storageClassName: fast
            resources:
                requests:
                    storage: 100Mi #1Mi是1024字节，1M是是1000字节
            accessModes:
            -   ReadWriteOnce
        ```
    * 不指定存储类，即无storageClassName属性时候，将使用默认的SC（即standard）。所以之前使用```storageClassName： ""```来进行PVC和PV的手动绑定，而那里的PVC和PV的storageClass为None而非fast或者standard


#### 附录
用docker构造fortune镜像
```shell
$ mkdir fortune #创建并进入
$ cd ./fortune

$ vi fortuneloop.sh #创建bash脚本并写入
#!/bin/bash
trap ” exit ” SIGINT
mkdir /var/htdocs
while
do
    echo $(date} Writing fortune to /var/htdocs/index .html
    /usr/games/fortune > /var/htdocs/index.html
    sleep 10
done

$ vi Dockerfile #创建Dockerfile
FROM ubuntu:latest #初始镜像ubuntu
RUN apt-get update ; apt-get -y install fortune #下载fortune
ADD fortuneloop .sh /bin/ fortuneloop.sh #将脚本加入文件
ENTRYPOINT / bin/ fortuneloop.sh #镜像启动时执行的命令

$ sudo docker build -t luksa/fortune .
$ sudo dockerpush luksa/fortune
```
